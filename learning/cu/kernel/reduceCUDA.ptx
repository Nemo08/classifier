//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	reduce
.global .align 4 .u32 hashCounter;
.global .align 8 .u64 alpha0;
.global .align 8 .u64 alpha1;

.visible .entry reduce(
	.param .u64 reduce_param_0,
	.param .u64 reduce_param_1,
	.param .u64 reduce_param_2,
	.param .u64 reduce_param_3,
	.param .u64 reduce_param_4,
	.param .u64 reduce_param_5
)
{
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<74>;
	.reg .b32 	%r<474>;
	.reg .b64 	%rd<175>;


	ld.param.u64 	%rd35, [reduce_param_0];
	ld.param.u64 	%rd36, [reduce_param_1];
	ld.param.u64 	%rd174, [reduce_param_2];
	ld.param.u64 	%rd173, [reduce_param_3];
	ld.param.u64 	%rd37, [reduce_param_5];
	cvta.to.global.u64 	%rd1, %rd37;
	cvta.to.global.u64 	%rd2, %rd35;
	cvta.to.global.u64 	%rd3, %rd36;
	ld.global.u32 	%r1, [%rd3+20];
	st.global.u64 	[alpha0], %rd174;
	st.global.u64 	[alpha1], %rd173;
	mov.u32 	%r88, %ntid.z;
	mov.u32 	%r89, %ctaid.z;
	mov.u32 	%r90, %tid.z;
	mad.lo.s32 	%r91, %r89, %r88, %r90;
	mov.u32 	%r92, %nctaid.y;
	mov.u32 	%r93, %ctaid.y;
	mad.lo.s32 	%r94, %r91, %r92, %r93;
	mov.u32 	%r95, %ntid.y;
	mov.u32 	%r96, %tid.y;
	mad.lo.s32 	%r97, %r94, %r95, %r96;
	mov.u32 	%r98, %nctaid.x;
	mov.u32 	%r99, %ctaid.x;
	mad.lo.s32 	%r100, %r97, %r98, %r99;
	mov.u32 	%r101, %ntid.x;
	mov.u32 	%r102, %tid.x;
	mad.lo.s32 	%r2, %r100, %r101, %r102;
	ld.global.u32 	%r103, [%rd3+16];
	setp.le.u32 	%p1, %r103, %r2;
	@%p1 bra 	$L__BB0_75;

	ld.global.u32 	%r104, [%rd3+12];
	add.s32 	%r3, %r104, 1;
	setp.eq.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB0_72;

	ld.global.u32 	%r106, [%rd3];
	add.s32 	%r107, %r106, 3;
	shr.u32 	%r108, %r107, 2;
	add.s32 	%r109, %r108, 4;
	ld.global.u32 	%r4, [%rd3+44];
	mul.lo.s32 	%r110, %r109, %r2;
	cvt.u64.u32 	%rd4, %r110;
	and.b32  	%r5, %r109, 3;
	sub.s32 	%r6, %r109, %r5;
	mov.u32 	%r440, 0;
	add.s64 	%rd38, %rd3, 52;
	add.s64 	%rd39, %rd3, 28;

$L__BB0_3:
	atom.global.add.u32 	%r111, [%rd38], 0;
	setp.gt.u32 	%p3, %r111, %r1;
	@%p3 bra 	$L__BB0_74;

	ld.global.u64 	%rd174, [alpha0];
	ld.global.u64 	%rd173, [alpha1];
	ld.global.u32 	%rd7, [%rd3];
	ld.global.u32 	%r8, [%rd3+4];
	ld.global.u32 	%r9, [%rd3+8];
	ld.global.u32 	%r10, [%rd3+36];
	atom.global.add.u32 	%r112, [%rd39], 0;
	setp.ge.u32 	%p4, %r112, %r10;
	@%p4 bra 	$L__BB0_74;

	ld.global.u32 	%r11, [%rd3+24];
	ld.global.u32 	%r12, [%rd3+40];
	ld.global.u32 	%r13, [%rd3+60];
	setp.ne.s32 	%p5, %r8, 1;
	setp.ne.s32 	%p6, %r9, 1;
	or.pred  	%p7, %p5, %p6;
	@%p7 bra 	$L__BB0_8;

	ld.u32 	%r113, [%rd174];
	setp.ne.s32 	%p8, %r113, 0;
	@%p8 bra 	$L__BB0_8;

	ld.u32 	%r114, [%rd173];
	setp.eq.s32 	%p9, %r114, 1;
	@%p9 bra 	$L__BB0_74;

$L__BB0_8:
	mov.u64 	%rd40, hashCounter;
	atom.global.add.u32 	%r116, [%rd40], 1;
	add.s32 	%r117, %r116, %r12;
	xor.b32  	%r14, %r117, %r11;
	mov.u32 	%r441, 0;
	mov.u32 	%r442, %r6;

$L__BB0_9:
	cvt.u64.u32 	%rd41, %r441;
	add.s64 	%rd42, %rd41, %rd4;
	add.s64 	%rd43, %rd2, %rd42;
	mov.u16 	%rs12, 0;
	st.global.u8 	[%rd43], %rs12;
	st.global.u8 	[%rd43+1], %rs12;
	st.global.u8 	[%rd43+2], %rs12;
	st.global.u8 	[%rd43+3], %rs12;
	add.s32 	%r441, %r441, 4;
	add.s32 	%r442, %r442, -4;
	setp.ne.s32 	%p10, %r442, 0;
	@%p10 bra 	$L__BB0_9;

	setp.eq.s32 	%p11, %r5, 0;
	@%p11 bra 	$L__BB0_14;

	setp.eq.s32 	%p12, %r5, 1;
	cvt.u64.u32 	%rd44, %r441;
	add.s64 	%rd45, %rd44, %rd4;
	add.s64 	%rd8, %rd2, %rd45;
	mov.u16 	%rs13, 0;
	st.global.u8 	[%rd8], %rs13;
	@%p12 bra 	$L__BB0_14;

	setp.eq.s32 	%p13, %r5, 2;
	st.global.u8 	[%rd8+1], %rs13;
	@%p13 bra 	$L__BB0_14;

	mov.u16 	%rs15, 0;
	st.global.u8 	[%rd8+2], %rs15;

$L__BB0_14:
	atom.global.add.u32 	%r118, [%rd38], 0;
	setp.gt.u32 	%p14, %r118, %r1;
	@%p14 bra 	$L__BB0_74;

	min.u32 	%r19, %r8, %r9;
	setp.eq.s32 	%p15, %r19, 0;
	mov.u16 	%rs71, 0;
	mov.u32 	%r467, 0;
	mov.u32 	%r458, %r467;
	@%p15 bra 	$L__BB0_21;

	mov.u32 	%r443, %r467;

$L__BB0_17:
	ld.global.u64 	%rd47, [alpha0];
	cvt.u64.u32 	%rd9, %r443;
	mul.wide.u32 	%rd48, %r443, 4;
	add.s64 	%rd49, %rd47, %rd48;
	ld.u32 	%r124, [%rd49];
	sub.s32 	%r125, %r124, %r14;
	shl.b32 	%r126, %r125, 2;
	xor.b32  	%r127, %r126, %r125;
	shl.b32 	%r128, %r127, 3;
	mov.u32 	%r129, 3;
	xor.b32  	%r130, %r128, %r127;
	shr.u32 	%r131, %r130, 5;
	xor.b32  	%r132, %r131, %r130;
	shr.u32 	%r133, %r132, 7;
	xor.b32  	%r134, %r133, %r132;
	shl.b32 	%r135, %r134, 11;
	xor.b32  	%r136, %r135, %r134;
	shl.b32 	%r137, %r136, 13;
	xor.b32  	%r138, %r137, %r136;
	shr.u32 	%r139, %r138, 17;
	xor.b32  	%r140, %r139, %r138;
	shl.b32 	%r141, %r140, 19;
	xor.b32  	%r142, %r141, %r140;
	add.s32 	%r143, %r142, %r14;
	cvt.u64.u32 	%rd50, %r143;
	mul.lo.s64 	%rd51, %rd50, %rd7;
	shr.u64 	%rd52, %rd51, 32;
	cvt.u32.u64 	%r144, %rd52;
	shr.u64 	%rd53, %rd51, 34;
	shl.b32 	%r145, %r144, 1;
	and.b32  	%r146, %r145, 6;
	add.s64 	%rd54, %rd53, %rd4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u8 	%rs18, [%rd55];
	cvt.u32.u16 	%r147, %rs18;
	shl.b32 	%r148, %r129, %r146;
	and.b32  	%r149, %r148, %r147;
	and.b32  	%r150, %r149, 255;
	setp.eq.s32 	%p16, %r150, 0;
	selp.u32 	%r151, 1, 0, %p16;
	add.s32 	%r458, %r458, %r151;
	mov.u16 	%rs71, 1;
	shl.b16 	%rs19, %rs71, %r146;
	or.b16  	%rs20, %rs18, %rs19;
	and.b16  	%rs21, %rs20, 254;
	st.global.u8 	[%rd55], %rs20;
	shr.u16 	%rs22, %rs21, 1;
	and.b16  	%rs23, %rs22, %rs20;
	setp.ne.s16 	%p17, %rs23, 0;
	@%p17 bra 	$L__BB0_19;

	ld.global.u64 	%rd56, [alpha1];
	shl.b64 	%rd57, %rd9, 2;
	add.s64 	%rd58, %rd56, %rd57;
	ld.u32 	%r152, [%rd58];
	sub.s32 	%r153, %r152, %r14;
	shl.b32 	%r154, %r153, 2;
	xor.b32  	%r155, %r154, %r153;
	shl.b32 	%r156, %r155, 3;
	xor.b32  	%r158, %r156, %r155;
	shr.u32 	%r159, %r158, 5;
	xor.b32  	%r160, %r159, %r158;
	shr.u32 	%r161, %r160, 7;
	xor.b32  	%r162, %r161, %r160;
	shl.b32 	%r163, %r162, 11;
	xor.b32  	%r164, %r163, %r162;
	shl.b32 	%r165, %r164, 13;
	xor.b32  	%r166, %r165, %r164;
	shr.u32 	%r167, %r166, 17;
	xor.b32  	%r168, %r167, %r166;
	shl.b32 	%r169, %r168, 19;
	xor.b32  	%r170, %r169, %r168;
	add.s32 	%r171, %r170, %r14;
	cvt.u64.u32 	%rd59, %r171;
	mul.lo.s64 	%rd60, %rd59, %rd7;
	shr.u64 	%rd61, %rd60, 32;
	cvt.u32.u64 	%r172, %rd61;
	shr.u64 	%rd62, %rd60, 34;
	shl.b32 	%r173, %r172, 1;
	and.b32  	%r174, %r173, 6;
	add.s64 	%rd63, %rd62, %rd4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.u8 	%rs24, [%rd64];
	cvt.u32.u16 	%r175, %rs24;
	shl.b32 	%r176, %r129, %r174;
	and.b32  	%r177, %r176, %r175;
	and.b32  	%r178, %r177, 255;
	setp.eq.s32 	%p18, %r178, 0;
	selp.u32 	%r179, 1, 0, %p18;
	add.s32 	%r467, %r467, %r179;
	mov.u16 	%rs25, 2;
	shl.b16 	%rs26, %rs25, %r174;
	or.b16  	%rs27, %rs24, %rs26;
	and.b16  	%rs28, %rs27, 254;
	st.global.u8 	[%rd64], %rs27;
	shr.u16 	%rs29, %rs28, 1;
	and.b16  	%rs30, %rs29, %rs27;
	setp.ne.s16 	%p19, %rs30, 0;
	selp.u16 	%rs71, 1, 0, %p19;

$L__BB0_19:
	cvt.u32.u64 	%r180, %rd9;
	add.s32 	%r443, %r180, 1;
	setp.ne.s16 	%p20, %rs71, 0;
	@%p20 bra 	$L__BB0_21;

	mov.u16 	%rs71, 0;
	min.u32 	%r439, %r8, %r9;
	setp.lt.u32 	%p21, %r443, %r439;
	@%p21 bra 	$L__BB0_17;

$L__BB0_21:
	setp.ne.s16 	%p22, %rs71, 0;
	@%p22 bra 	$L__BB0_71;

	atom.global.add.u32 	%r181, [%rd38], 0;
	setp.gt.u32 	%p23, %r181, %r1;
	@%p23 bra 	$L__BB0_74;

	setp.le.u32 	%p24, %r8, %r9;
	mov.u16 	%rs73, 0;
	@%p24 bra 	$L__BB0_27;

	min.u32 	%r449, %r8, %r9;

$L__BB0_25:
	ld.global.u64 	%rd66, [alpha0];
	mul.wide.u32 	%rd67, %r449, 4;
	add.s64 	%rd68, %rd66, %rd67;
	ld.u32 	%r182, [%rd68];
	sub.s32 	%r183, %r182, %r14;
	shl.b32 	%r184, %r183, 2;
	xor.b32  	%r185, %r184, %r183;
	shl.b32 	%r186, %r185, 3;
	mov.u32 	%r187, 3;
	xor.b32  	%r188, %r186, %r185;
	shr.u32 	%r189, %r188, 5;
	xor.b32  	%r190, %r189, %r188;
	shr.u32 	%r191, %r190, 7;
	xor.b32  	%r192, %r191, %r190;
	shl.b32 	%r193, %r192, 11;
	xor.b32  	%r194, %r193, %r192;
	shl.b32 	%r195, %r194, 13;
	xor.b32  	%r196, %r195, %r194;
	shr.u32 	%r197, %r196, 17;
	xor.b32  	%r198, %r197, %r196;
	shl.b32 	%r199, %r198, 19;
	xor.b32  	%r200, %r199, %r198;
	add.s32 	%r201, %r200, %r14;
	cvt.u64.u32 	%rd69, %r201;
	mul.lo.s64 	%rd70, %rd69, %rd7;
	shr.u64 	%rd71, %rd70, 32;
	cvt.u32.u64 	%r202, %rd71;
	shr.u64 	%rd72, %rd70, 34;
	shl.b32 	%r203, %r202, 1;
	and.b32  	%r204, %r203, 6;
	add.s64 	%rd73, %rd72, %rd4;
	add.s64 	%rd74, %rd2, %rd73;
	ld.global.u8 	%rs34, [%rd74];
	cvt.u32.u16 	%r205, %rs34;
	shl.b32 	%r206, %r187, %r204;
	and.b32  	%r207, %r206, %r205;
	and.b32  	%r208, %r207, 255;
	setp.eq.s32 	%p25, %r208, 0;
	selp.u32 	%r209, 1, 0, %p25;
	add.s32 	%r458, %r458, %r209;
	mov.u16 	%rs73, 1;
	shl.b16 	%rs35, %rs73, %r204;
	or.b16  	%rs36, %rs34, %rs35;
	and.b16  	%rs37, %rs36, 254;
	st.global.u8 	[%rd74], %rs36;
	shr.u16 	%rs38, %rs37, 1;
	and.b16  	%rs39, %rs38, %rs36;
	setp.ne.s16 	%p26, %rs39, 0;
	add.s32 	%r449, %r449, 1;
	@%p26 bra 	$L__BB0_27;

	mov.u16 	%rs73, 0;
	setp.lt.u32 	%p27, %r449, %r8;
	@%p27 bra 	$L__BB0_25;

$L__BB0_27:
	setp.ge.u32 	%p28, %r8, %r9;
	setp.ne.s16 	%p29, %rs73, 0;
	or.pred  	%p30, %p29, %p28;
	@%p30 bra 	$L__BB0_31;

	min.u32 	%r452, %r8, %r9;

$L__BB0_29:
	ld.global.u64 	%rd75, [alpha1];
	mul.wide.u32 	%rd76, %r452, 4;
	add.s64 	%rd77, %rd75, %rd76;
	ld.u32 	%r210, [%rd77];
	sub.s32 	%r211, %r210, %r14;
	shl.b32 	%r212, %r211, 2;
	xor.b32  	%r213, %r212, %r211;
	shl.b32 	%r214, %r213, 3;
	mov.u32 	%r215, 3;
	xor.b32  	%r216, %r214, %r213;
	shr.u32 	%r217, %r216, 5;
	xor.b32  	%r218, %r217, %r216;
	shr.u32 	%r219, %r218, 7;
	xor.b32  	%r220, %r219, %r218;
	shl.b32 	%r221, %r220, 11;
	xor.b32  	%r222, %r221, %r220;
	shl.b32 	%r223, %r222, 13;
	xor.b32  	%r224, %r223, %r222;
	shr.u32 	%r225, %r224, 17;
	xor.b32  	%r226, %r225, %r224;
	shl.b32 	%r227, %r226, 19;
	xor.b32  	%r228, %r227, %r226;
	add.s32 	%r229, %r228, %r14;
	cvt.u64.u32 	%rd78, %r229;
	mul.lo.s64 	%rd79, %rd78, %rd7;
	shr.u64 	%rd80, %rd79, 32;
	cvt.u32.u64 	%r230, %rd80;
	shr.u64 	%rd81, %rd79, 34;
	shl.b32 	%r231, %r230, 1;
	and.b32  	%r232, %r231, 6;
	add.s64 	%rd82, %rd81, %rd4;
	add.s64 	%rd83, %rd2, %rd82;
	ld.global.u8 	%rs42, [%rd83];
	cvt.u32.u16 	%r233, %rs42;
	shl.b32 	%r234, %r215, %r232;
	and.b32  	%r235, %r234, %r233;
	and.b32  	%r236, %r235, 255;
	setp.eq.s32 	%p31, %r236, 0;
	selp.u32 	%r237, 1, 0, %p31;
	add.s32 	%r467, %r467, %r237;
	mov.u16 	%rs43, 2;
	shl.b16 	%rs44, %rs43, %r232;
	or.b16  	%rs45, %rs42, %rs44;
	and.b16  	%rs46, %rs45, 254;
	st.global.u8 	[%rd83], %rs45;
	shr.u16 	%rs47, %rs46, 1;
	and.b16  	%rs48, %rs47, %rs45;
	setp.ne.s16 	%p32, %rs48, 0;
	add.s32 	%r452, %r452, 1;
	mov.u16 	%rs73, 1;
	@%p32 bra 	$L__BB0_31;

	setp.lt.u32 	%p33, %r452, %r9;
	mov.u16 	%rs73, 0;
	@%p33 bra 	$L__BB0_29;

$L__BB0_31:
	setp.ne.s16 	%p34, %rs73, 0;
	@%p34 bra 	$L__BB0_71;

	add.s32 	%r238, %r458, %r467;
	setp.ne.s32 	%p35, %r238, 2;
	add.s32 	%r239, %r9, %r8;
	setp.eq.s32 	%p36, %r239, %r238;
	and.pred  	%p37, %p36, %p35;
	@%p37 bra 	$L__BB0_71;

	atom.global.add.u32 	%r240, [%rd38], 0;
	setp.gt.u32 	%p38, %r240, %r1;
	@%p38 bra 	$L__BB0_74;

	ld.param.u64 	%rd169, [reduce_param_4];
	add.s64 	%rd85, %rd3, 56;
	atom.global.add.u32 	%r241, [%rd85], %r458;
	mul.wide.u32 	%rd86, %r241, 4;
	add.s64 	%rd87, %rd169, %rd86;
	setp.eq.s64 	%p39, %rd87, 0;
	add.s32 	%r242, %r241, %r458;
	setp.ge.u32 	%p40, %r242, %r4;
	or.pred  	%p41, %p40, %p39;
	selp.b64 	%rd11, 0, %rd87, %p41;
	atom.global.add.u32 	%r243, [%rd85], %r467;
	mul.wide.u32 	%rd88, %r243, 4;
	add.s64 	%rd89, %rd169, %rd88;
	setp.eq.s64 	%p42, %rd89, 0;
	add.s32 	%r244, %r243, %r467;
	setp.ge.u32 	%p43, %r244, %r4;
	or.pred  	%p44, %p43, %p42;
	selp.b64 	%rd12, 0, %rd89, %p44;
	setp.eq.s64 	%p45, %rd11, 0;
	setp.eq.s64 	%p46, %rd12, 0;
	or.pred  	%p47, %p45, %p46;
	@%p47 bra 	$L__BB0_57;

	setp.eq.s32 	%p48, %r8, 0;
	mov.u32 	%r467, 0;
	mov.u32 	%r458, %r467;
	@%p48 bra 	$L__BB0_46;

	setp.eq.s32 	%p49, %r8, 1;
	and.b32  	%r39, %r8, 1;
	mov.u32 	%r461, 0;
	mov.u32 	%r458, %r461;
	@%p49 bra 	$L__BB0_43;

	sub.s32 	%r457, %r8, %r39;

$L__BB0_38:
	ld.global.u64 	%rd170, [alpha0];
	mul.wide.u32 	%rd90, %r461, 4;
	add.s64 	%rd91, %rd170, %rd90;
	ld.u32 	%r251, [%rd91];
	sub.s32 	%r252, %r251, %r14;
	shl.b32 	%r253, %r252, 2;
	xor.b32  	%r254, %r253, %r252;
	shl.b32 	%r255, %r254, 3;
	xor.b32  	%r256, %r255, %r254;
	shr.u32 	%r257, %r256, 5;
	xor.b32  	%r258, %r257, %r256;
	shr.u32 	%r259, %r258, 7;
	xor.b32  	%r260, %r259, %r258;
	shl.b32 	%r261, %r260, 11;
	xor.b32  	%r262, %r261, %r260;
	shl.b32 	%r263, %r262, 13;
	xor.b32  	%r264, %r263, %r262;
	shr.u32 	%r265, %r264, 17;
	xor.b32  	%r266, %r265, %r264;
	shl.b32 	%r267, %r266, 19;
	xor.b32  	%r268, %r267, %r266;
	add.s32 	%r269, %r268, %r14;
	cvt.u64.u32 	%rd92, %r269;
	mul.lo.s64 	%rd93, %rd92, %rd7;
	shr.u64 	%rd94, %rd93, 32;
	cvt.u32.u64 	%r44, %rd94;
	shr.u64 	%rd95, %rd93, 34;
	shl.b32 	%r270, %r44, 1;
	and.b32  	%r45, %r270, 6;
	add.s64 	%rd96, %rd95, %rd4;
	add.s64 	%rd14, %rd2, %rd96;
	ld.global.u8 	%rs6, [%rd14];
	cvt.u32.u16 	%r271, %rs6;
	and.b32  	%r272, %r271, 255;
	shr.u32 	%r273, %r272, %r45;
	and.b32  	%r274, %r273, 3;
	setp.eq.s32 	%p50, %r274, 3;
	@%p50 bra 	$L__BB0_40;

	mov.u16 	%rs50, 3;
	shl.b16 	%rs51, %rs50, %r45;
	or.b16  	%rs52, %rs6, %rs51;
	st.global.u8 	[%rd14], %rs52;
	add.s32 	%r46, %r458, 1;
	mul.wide.u32 	%rd97, %r458, 4;
	add.s64 	%rd98, %rd11, %rd97;
	st.u32 	[%rd98], %r44;
	ld.global.u64 	%rd170, [alpha0];
	mov.u32 	%r458, %r46;

$L__BB0_40:
	add.s32 	%r275, %r461, 1;
	mul.wide.u32 	%rd99, %r275, 4;
	add.s64 	%rd100, %rd170, %rd99;
	ld.u32 	%r276, [%rd100];
	sub.s32 	%r277, %r276, %r14;
	shl.b32 	%r278, %r277, 2;
	xor.b32  	%r279, %r278, %r277;
	shl.b32 	%r280, %r279, 3;
	xor.b32  	%r281, %r280, %r279;
	shr.u32 	%r282, %r281, 5;
	xor.b32  	%r283, %r282, %r281;
	shr.u32 	%r284, %r283, 7;
	xor.b32  	%r285, %r284, %r283;
	shl.b32 	%r286, %r285, 11;
	xor.b32  	%r287, %r286, %r285;
	shl.b32 	%r288, %r287, 13;
	xor.b32  	%r289, %r288, %r287;
	shr.u32 	%r290, %r289, 17;
	xor.b32  	%r291, %r290, %r289;
	shl.b32 	%r292, %r291, 19;
	xor.b32  	%r293, %r292, %r291;
	add.s32 	%r294, %r293, %r14;
	cvt.u64.u32 	%rd101, %r294;
	mul.lo.s64 	%rd102, %rd101, %rd7;
	shr.u64 	%rd103, %rd102, 32;
	cvt.u32.u64 	%r48, %rd103;
	shr.u64 	%rd104, %rd102, 34;
	shl.b32 	%r295, %r48, 1;
	and.b32  	%r49, %r295, 6;
	add.s64 	%rd105, %rd104, %rd4;
	add.s64 	%rd17, %rd2, %rd105;
	ld.global.u8 	%rs7, [%rd17];
	cvt.u32.u16 	%r296, %rs7;
	and.b32  	%r297, %r296, 255;
	shr.u32 	%r298, %r297, %r49;
	and.b32  	%r299, %r298, 3;
	setp.eq.s32 	%p51, %r299, 3;
	@%p51 bra 	$L__BB0_42;

	mov.u16 	%rs53, 3;
	shl.b16 	%rs54, %rs53, %r49;
	or.b16  	%rs55, %rs7, %rs54;
	st.global.u8 	[%rd17], %rs55;
	add.s32 	%r50, %r458, 1;
	mul.wide.u32 	%rd106, %r458, 4;
	add.s64 	%rd107, %rd11, %rd106;
	st.u32 	[%rd107], %r48;
	mov.u32 	%r458, %r50;

$L__BB0_42:
	add.s32 	%r461, %r461, 2;
	add.s32 	%r457, %r457, -2;
	setp.ne.s32 	%p52, %r457, 0;
	@%p52 bra 	$L__BB0_38;

$L__BB0_43:
	setp.eq.s32 	%p53, %r39, 0;
	@%p53 bra 	$L__BB0_46;

	ld.global.u64 	%rd108, [alpha0];
	mul.wide.u32 	%rd109, %r461, 4;
	add.s64 	%rd110, %rd108, %rd109;
	ld.u32 	%r300, [%rd110];
	sub.s32 	%r301, %r300, %r14;
	shl.b32 	%r302, %r301, 2;
	xor.b32  	%r303, %r302, %r301;
	shl.b32 	%r304, %r303, 3;
	xor.b32  	%r305, %r304, %r303;
	shr.u32 	%r306, %r305, 5;
	xor.b32  	%r307, %r306, %r305;
	shr.u32 	%r308, %r307, 7;
	xor.b32  	%r309, %r308, %r307;
	shl.b32 	%r310, %r309, 11;
	xor.b32  	%r311, %r310, %r309;
	shl.b32 	%r312, %r311, 13;
	xor.b32  	%r313, %r312, %r311;
	shr.u32 	%r314, %r313, 17;
	xor.b32  	%r315, %r314, %r313;
	shl.b32 	%r316, %r315, 19;
	xor.b32  	%r317, %r316, %r315;
	add.s32 	%r318, %r317, %r14;
	cvt.u64.u32 	%rd111, %r318;
	mul.lo.s64 	%rd112, %rd111, %rd7;
	shr.u64 	%rd113, %rd112, 32;
	cvt.u32.u64 	%r57, %rd113;
	shr.u64 	%rd114, %rd112, 34;
	shl.b32 	%r319, %r57, 1;
	and.b32  	%r58, %r319, 6;
	add.s64 	%rd115, %rd114, %rd4;
	add.s64 	%rd18, %rd2, %rd115;
	ld.global.u8 	%rs8, [%rd18];
	cvt.u32.u16 	%r320, %rs8;
	and.b32  	%r321, %r320, 255;
	shr.u32 	%r322, %r321, %r58;
	and.b32  	%r323, %r322, 3;
	setp.eq.s32 	%p54, %r323, 3;
	@%p54 bra 	$L__BB0_46;

	mov.u16 	%rs56, 3;
	shl.b16 	%rs57, %rs56, %r58;
	or.b16  	%rs58, %rs8, %rs57;
	st.global.u8 	[%rd18], %rs58;
	add.s32 	%r59, %r458, 1;
	mul.wide.u32 	%rd116, %r458, 4;
	add.s64 	%rd117, %rd11, %rd116;
	st.u32 	[%rd117], %r57;
	mov.u32 	%r458, %r59;

$L__BB0_46:
	setp.eq.s32 	%p55, %r9, 0;
	@%p55 bra 	$L__BB0_57;

	setp.eq.s32 	%p56, %r9, 1;
	and.b32  	%r61, %r9, 1;
	mov.u32 	%r470, 0;
	mov.u32 	%r467, %r470;
	@%p56 bra 	$L__BB0_54;

	sub.s32 	%r466, %r9, %r61;

$L__BB0_49:
	ld.global.u64 	%rd171, [alpha1];
	mul.wide.u32 	%rd118, %r470, 4;
	add.s64 	%rd119, %rd171, %rd118;
	ld.u32 	%r330, [%rd119];
	sub.s32 	%r331, %r330, %r14;
	shl.b32 	%r332, %r331, 2;
	xor.b32  	%r333, %r332, %r331;
	shl.b32 	%r334, %r333, 3;
	xor.b32  	%r335, %r334, %r333;
	shr.u32 	%r336, %r335, 5;
	xor.b32  	%r337, %r336, %r335;
	shr.u32 	%r338, %r337, 7;
	xor.b32  	%r339, %r338, %r337;
	shl.b32 	%r340, %r339, 11;
	xor.b32  	%r341, %r340, %r339;
	shl.b32 	%r342, %r341, 13;
	xor.b32  	%r343, %r342, %r341;
	shr.u32 	%r344, %r343, 17;
	xor.b32  	%r345, %r344, %r343;
	shl.b32 	%r346, %r345, 19;
	xor.b32  	%r347, %r346, %r345;
	add.s32 	%r348, %r347, %r14;
	cvt.u64.u32 	%rd120, %r348;
	mul.lo.s64 	%rd121, %rd120, %rd7;
	shr.u64 	%rd122, %rd121, 32;
	cvt.u32.u64 	%r66, %rd122;
	shr.u64 	%rd123, %rd121, 34;
	shl.b32 	%r349, %r66, 1;
	and.b32  	%r67, %r349, 6;
	add.s64 	%rd124, %rd123, %rd4;
	add.s64 	%rd20, %rd2, %rd124;
	ld.global.u8 	%rs9, [%rd20];
	cvt.u32.u16 	%r350, %rs9;
	and.b32  	%r351, %r350, 255;
	shr.u32 	%r352, %r351, %r67;
	and.b32  	%r353, %r352, 3;
	setp.eq.s32 	%p57, %r353, 3;
	@%p57 bra 	$L__BB0_51;

	mov.u16 	%rs59, 3;
	shl.b16 	%rs60, %rs59, %r67;
	or.b16  	%rs61, %rs9, %rs60;
	st.global.u8 	[%rd20], %rs61;
	add.s32 	%r68, %r467, 1;
	mul.wide.u32 	%rd125, %r467, 4;
	add.s64 	%rd126, %rd12, %rd125;
	st.u32 	[%rd126], %r66;
	ld.global.u64 	%rd171, [alpha1];
	mov.u32 	%r467, %r68;

$L__BB0_51:
	add.s32 	%r354, %r470, 1;
	mul.wide.u32 	%rd127, %r354, 4;
	add.s64 	%rd128, %rd171, %rd127;
	ld.u32 	%r355, [%rd128];
	sub.s32 	%r356, %r355, %r14;
	shl.b32 	%r357, %r356, 2;
	xor.b32  	%r358, %r357, %r356;
	shl.b32 	%r359, %r358, 3;
	xor.b32  	%r360, %r359, %r358;
	shr.u32 	%r361, %r360, 5;
	xor.b32  	%r362, %r361, %r360;
	shr.u32 	%r363, %r362, 7;
	xor.b32  	%r364, %r363, %r362;
	shl.b32 	%r365, %r364, 11;
	xor.b32  	%r366, %r365, %r364;
	shl.b32 	%r367, %r366, 13;
	xor.b32  	%r368, %r367, %r366;
	shr.u32 	%r369, %r368, 17;
	xor.b32  	%r370, %r369, %r368;
	shl.b32 	%r371, %r370, 19;
	xor.b32  	%r372, %r371, %r370;
	add.s32 	%r373, %r372, %r14;
	cvt.u64.u32 	%rd129, %r373;
	mul.lo.s64 	%rd130, %rd129, %rd7;
	shr.u64 	%rd131, %rd130, 32;
	cvt.u32.u64 	%r70, %rd131;
	shr.u64 	%rd132, %rd130, 34;
	shl.b32 	%r374, %r70, 1;
	and.b32  	%r71, %r374, 6;
	add.s64 	%rd133, %rd132, %rd4;
	add.s64 	%rd23, %rd2, %rd133;
	ld.global.u8 	%rs10, [%rd23];
	cvt.u32.u16 	%r375, %rs10;
	and.b32  	%r376, %r375, 255;
	shr.u32 	%r377, %r376, %r71;
	and.b32  	%r378, %r377, 3;
	setp.eq.s32 	%p58, %r378, 3;
	@%p58 bra 	$L__BB0_53;

	mov.u16 	%rs62, 3;
	shl.b16 	%rs63, %rs62, %r71;
	or.b16  	%rs64, %rs10, %rs63;
	st.global.u8 	[%rd23], %rs64;
	add.s32 	%r72, %r467, 1;
	mul.wide.u32 	%rd134, %r467, 4;
	add.s64 	%rd135, %rd12, %rd134;
	st.u32 	[%rd135], %r70;
	mov.u32 	%r467, %r72;

$L__BB0_53:
	add.s32 	%r470, %r470, 2;
	add.s32 	%r466, %r466, -2;
	setp.ne.s32 	%p59, %r466, 0;
	@%p59 bra 	$L__BB0_49;

$L__BB0_54:
	setp.eq.s32 	%p60, %r61, 0;
	@%p60 bra 	$L__BB0_57;

	ld.global.u64 	%rd136, [alpha1];
	mul.wide.u32 	%rd137, %r470, 4;
	add.s64 	%rd138, %rd136, %rd137;
	ld.u32 	%r379, [%rd138];
	sub.s32 	%r380, %r379, %r14;
	shl.b32 	%r381, %r380, 2;
	xor.b32  	%r382, %r381, %r380;
	shl.b32 	%r383, %r382, 3;
	xor.b32  	%r384, %r383, %r382;
	shr.u32 	%r385, %r384, 5;
	xor.b32  	%r386, %r385, %r384;
	shr.u32 	%r387, %r386, 7;
	xor.b32  	%r388, %r387, %r386;
	shl.b32 	%r389, %r388, 11;
	xor.b32  	%r390, %r389, %r388;
	shl.b32 	%r391, %r390, 13;
	xor.b32  	%r392, %r391, %r390;
	shr.u32 	%r393, %r392, 17;
	xor.b32  	%r394, %r393, %r392;
	shl.b32 	%r395, %r394, 19;
	xor.b32  	%r396, %r395, %r394;
	add.s32 	%r397, %r396, %r14;
	cvt.u64.u32 	%rd139, %r397;
	mul.lo.s64 	%rd140, %rd139, %rd7;
	shr.u64 	%rd141, %rd140, 32;
	cvt.u32.u64 	%r79, %rd141;
	shr.u64 	%rd142, %rd140, 34;
	shl.b32 	%r398, %r79, 1;
	and.b32  	%r80, %r398, 6;
	add.s64 	%rd143, %rd142, %rd4;
	add.s64 	%rd24, %rd2, %rd143;
	ld.global.u8 	%rs11, [%rd24];
	cvt.u32.u16 	%r399, %rs11;
	and.b32  	%r400, %r399, 255;
	shr.u32 	%r401, %r400, %r80;
	and.b32  	%r402, %r401, 3;
	setp.eq.s32 	%p61, %r402, 3;
	@%p61 bra 	$L__BB0_57;

	mov.u16 	%rs65, 3;
	shl.b16 	%rs66, %rs65, %r80;
	or.b16  	%rs67, %rs11, %rs66;
	st.global.u8 	[%rd24], %rs67;
	add.s32 	%r81, %r467, 1;
	mul.wide.u32 	%rd144, %r467, 4;
	add.s64 	%rd145, %rd12, %rd144;
	st.u32 	[%rd145], %r79;
	mov.u32 	%r467, %r81;

$L__BB0_57:
	add.s64 	%rd146, %rd3, 48;
	mov.u32 	%r403, 1;
	mov.u32 	%r404, 0;
	atom.global.cas.b32 	%r405, [%rd146], %r404, %r403;
	setp.eq.s32 	%p62, %r405, 0;
	@%p62 bra 	$L__BB0_60;

	atom.global.cas.b32 	%r408, [%rd146], %r404, %r403;
	setp.eq.s32 	%p63, %r408, 0;
	@%p63 bra 	$L__BB0_60;

	mov.u32 	%r409, 1;
	mov.u32 	%r410, 0;
	atom.global.cas.b32 	%r411, [%rd146], %r410, %r409;
	setp.ne.s32 	%p64, %r411, 0;
	@%p64 bra 	$L__BB0_71;

$L__BB0_60:
	atom.global.add.u32 	%r412, [%rd38], 0;
	setp.gt.u32 	%p65, %r412, %r1;
	@%p65 bra 	$L__BB0_72;

	atom.global.add.u32 	%r84, [%rd39], 2;
	setp.ge.u32 	%p66, %r84, %r10;
	@%p66 bra 	$L__BB0_72;

	setp.eq.s32 	%p67, %r84, 0;
	@%p67 bra 	$L__BB0_64;

	cvt.u32.u64 	%r413, %rd7;
	add.s32 	%r414, %r84, -1;
	mul.wide.u32 	%rd151, %r414, 4;
	add.s64 	%rd152, %rd1, %rd151;
	atom.global.add.u32 	%r415, [%rd152], 0;
	setp.le.u32 	%p68, %r415, %r413;
	@%p68 bra 	$L__BB0_72;

$L__BB0_64:
	cvt.u32.u64 	%r416, %rd7;
	mul.wide.u32 	%rd153, %r84, 4;
	add.s64 	%rd154, %rd1, %rd153;
	atom.global.add.u32 	%r417, [%rd154], %r14;
	add.s32 	%r418, %r84, 1;
	mul.wide.u32 	%rd155, %r418, 4;
	add.s64 	%rd156, %rd1, %rd155;
	atom.global.add.u32 	%r419, [%rd156], %r416;
	add.s32 	%r420, %r84, 2;
	setp.ge.u32 	%p69, %r420, %r10;
	@%p69 bra 	$L__BB0_72;

	setp.eq.s64 	%p84, %rd12, 0;
	setp.eq.s64 	%p83, %rd11, 0;
	or.pred  	%p82, %p83, %p84;
	@%p82 bra 	$L__BB0_72;

	max.u32 	%r85, %r458, %r467;
	setp.gt.u32 	%p73, %r13, %r85;
	add.s32 	%r421, %r85, -1;
	selp.b32 	%r422, %r421, %r13, %p73;
	sub.s32 	%r423, %r85, %r422;
	cvt.u64.u32 	%rd157, %r423;
	mul.lo.s64 	%rd158, %rd157, %rd7;
	mul.lo.s64 	%rd25, %rd158, %rd157;
	mul.wide.u32 	%rd26, %r85, %r85;
	or.b64  	%rd159, %rd25, %rd26;
	and.b64  	%rd160, %rd159, -4294967296;
	setp.eq.s64 	%p74, %rd160, 0;
	@%p74 bra 	$L__BB0_68;

	div.u64 	%rd172, %rd25, %rd26;
	bra.uni 	$L__BB0_69;

$L__BB0_68:
	cvt.u32.u64 	%r424, %rd26;
	cvt.u32.u64 	%r425, %rd25;
	div.u32 	%r426, %r425, %r424;
	cvt.u64.u32 	%rd172, %r426;

$L__BB0_69:
	cvt.u32.u64 	%r86, %rd172;
	setp.lt.u32 	%p75, %r86, 2;
	setp.le.u32 	%p76, %r416, %r86;
	or.pred  	%p77, %p75, %p76;
	@%p77 bra 	$L__BB0_72;

	max.u32 	%r428, %r85, %r86;
	atom.global.exch.b32 	%r429, [%rd3], %r428;
	add.s64 	%rd161, %rd3, 4;
	atom.global.exch.b32 	%r430, [%rd161], %r458;
	add.s64 	%rd162, %rd3, 8;
	atom.global.exch.b32 	%r431, [%rd162], %r467;
	add.s64 	%rd163, %rd3, 24;
	atom.global.exch.b32 	%r432, [%rd163], %r14;
	st.global.u64 	[alpha0], %rd11;
	st.global.u64 	[alpha1], %rd12;
	atom.global.exch.b32 	%r433, [%rd146], 0;

$L__BB0_71:
	add.s32 	%r440, %r440, 1;
	setp.lt.u32 	%p78, %r440, %r3;
	@%p78 bra 	$L__BB0_3;

$L__BB0_72:
	add.s64 	%rd165, %rd3, 48;
	atom.global.exch.b32 	%r434, [%rd165], 0;
	ld.global.u64 	%rd166, [alpha0];
	setp.eq.s64 	%p79, %rd174, %rd166;
	ld.global.u64 	%rd167, [alpha1];
	setp.eq.s64 	%p80, %rd173, %rd167;
	or.pred  	%p81, %p79, %p80;
	@%p81 bra 	$L__BB0_74;

	st.global.u64 	[alpha0], %rd174;
	st.global.u64 	[alpha1], %rd173;

$L__BB0_74:
	add.s64 	%rd168, %rd3, 52;
	add.s32 	%r435, %r1, 1;
	atom.global.add.u32 	%r436, [%rd168], %r435;

$L__BB0_75:
	ret;

}

